<!DOCTYPE html> 
<link rel="stylesheet" href="https://cdn.rawgit.com/Chalarangelo/mini.css/v3.0.1/dist/mini-default.min.css">
<meta name="viewport" content="width=device-width, initial-scale=1">
<html>
<head>
<style>
div {margin: auto;
  width: 40%;
  min-width:300px;
}

</style>
<title>Postdoc Research | Wenchao Cao</title>
<ul>
<li class="current menu0"><a href='https://wenchao320.github.io/index.html'>About</a></li>
<li class="menu1"><a href='https://wenchao320.github.io/mayo.html'>Mayo Clinic</a></li>
<li class="menu2"><a href='https://wenchao320.github.io/research_postdoc.html'>Upenn</a></li>
<li class="menu3"><a href='https://wenchao320.github.io/research.html'>PhD Research</a></li>
<li class="menu4"><a href='https://wenchao320.github.io/courses.html'>Courses</a></li>
<li class="menu5"><a href='https://wenchao320.github.io/contact.html'>Contact</a></li>
</ul>
</head>

<div>
<h1>Mayo Clinic </h1>
<p style="text-align: left;">My work at Mayo Clinic focuses on delivering clinical impact with AI-assisted CT image processing. Specific problems faced by CT imaging are: 1) Image processing on the raw data often yields better results. The raw data, however, is not always available. Reconstruction domain algorithms are therefore more likely to be successfully deployed into clinical practice. 2) Although larger datasets often means better generalizability, pre-trained model on large datasets may not yield optimal results for a specific case of CT reconstruction, as various conditions (imaging protocols, patient conditions, reconstruction kernels etc.) may cause deviations from the training data distribution. This affects the performance of the pre-trained CNN. 

Through the use of finetuning on patient-specific data from pre-trained models on large data, we aim to achieve improved results with efficiency.
  </p>
  
<h2>Patient-specific uniform patch based denoising</h2>
<figure style="text-align: center;">
  <img style="margin-right: auto; margin-left: auto; display: block;" src="https://lh3.googleusercontent.com/pw/AM-JKLW2v48SYoibsJDRo3u35UhaziNY50JTS9OVKyVPxt4weC7bLUFaEkV5F8z_IVqkx5g0ak_i1nLkA-uyJXOpuj8EidcZObiHNVIuElrqs-2EPw5rrZs6F3VubF_rJX_05TzSgv6UTgsXYEx2Xy7D5bc7=w1150-h569-no?authuser=0" />
  <figcaption>Results using 100 noise patches without and with transfer learning </figcaption>
    </figure>
<p style="text-align: left;">Our preliminary results indicate that the denoising CNN did not generalize well between FBP reconstruction kernels, but is insensitive to dose levels. The goal of this project is to develop a generalizable patient-specific image domain CT denoising method. The hypothesis is a limited number of noise patches from the patient reconstruction is sufficient for generating noisemaps to fine-tune a CNN to denoise the specific patient. Specifically, we aim to investigate the following process: 1. For a single patient, generate noise prediction using uniform patches from the patient reconstruction; 2. Break patient image into patches (e.g. $64\times64\times5$) </p>

<footer>
<p class="copy">Wenchao Cao (曹文超) &copy; 2021</p>

</footer></div>
